import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import random
from collections import deque
import matplotlib.pyplot as plt
import argparse
import os
from dataclasses import dataclass
from typing import List, Tuple, Dict, Optional
import warnings
warnings.filterwarnings('ignore')


# -------------------------- 1. è®ºæ–‡æ ¸å¿ƒçŠ¶æ€å®šä¹‰ï¼ˆåŸºäºğŸ”¶1-109ã€ğŸ”¶1-115ï¼‰ --------------------------
@dataclass
class CyberState:
    """
    è®ºæ–‡å®šä¹‰ï¼šå•é˜²å¾¡è€…-å•æ”»å‡»è€…åœºæ™¯çš„çŠ¶æ€ = è¿‡å»Tè½®æ”»é˜²ç­–ç•¥è§‚æµ‹çª—å£
    è§‚æµ‹o^{t-j} = (x^{t-j}, y^{t-j})ï¼Œå…¶ä¸­xä¸ºé˜²å¾¡è€…èµ„æºåˆ†é…ç­–ç•¥ï¼Œyä¸ºæ”»å‡»è€…èµ„æºåˆ†é…ç­–ç•¥
    å‚è€ƒğŸ”¶1-109ï¼šs_i^t = {o_i^{t-T}, ..., o_i^{t-1}}ï¼›å•æ™ºèƒ½ä½“åœºæ™¯ç®€åŒ–ä¸ºs^t = {o^{t-T}, ..., o^{t-1}}
    """
    history_strategies: List[Tuple[np.ndarray, np.ndarray]]  # å†å²ç­–ç•¥çª—å£ï¼š[(x_1,y_1), (x_2,y_2), ..., (x_T,y_T)]
    history_window: int  # Tï¼šå†å²çª—å£é•¿åº¦ï¼ˆè®ºæ–‡æœªæŒ‡å®šå›ºå®šå€¼ï¼Œå®éªŒä¸­å¯é…ç½®ï¼‰


# -------------------------- 2. è®ºæ–‡ç¯å¢ƒå»ºæ¨¡ï¼ˆåŸºäºğŸ”¶1-45ã€ğŸ”¶1-51ã€ğŸ”¶1-57ã€ğŸ”¶1-58ï¼‰ --------------------------
class CyberSecurityEnvironment:
    """
    è®ºæ–‡æ ¸å¿ƒï¼šå•é˜²å¾¡è€…-å•æ”»å‡»è€…çš„Colonel Blottoèµ„æºåˆ†é…åšå¼ˆç¯å¢ƒ
    å…³é”®é€»è¾‘ï¼šæ”»é˜²åŒæ–¹åŒæ—¶åˆ†é…èµ„æºâ†’å¯¹æ¯”æ¯å°ä¸»æœºèµ„æºâ†’è®¡ç®—æ§åˆ¶æƒâ†’å¾—æ•ˆç”¨
    å‚è€ƒğŸ”¶1-45ï¼ˆåšå¼ˆæ¨¡å‹ï¼‰ã€ğŸ”¶1-51ï¼ˆColonel Blottoæ‰©å±•ï¼‰ã€ğŸ”¶1-57ï¼ˆä¸»æœºæ§åˆ¶æƒè§„åˆ™ï¼‰ã€ğŸ”¶1-58ï¼ˆæ•ˆç”¨å…¬å¼ï¼‰
    """
    def __init__(self, config: Dict):
        # è®ºæ–‡æ ¸å¿ƒå‚æ•°ï¼ˆğŸ”¶1-46ã€ğŸ”¶1-49ã€ğŸ”¶1-50ï¼‰
        self.num_hosts = config["num_hosts"]  # Nï¼šç½‘ç»œä¸­ä¸»æœºæ•°é‡
        self.defender_total_res = config["defender_total_res"]  # Bï¼šé˜²å¾¡è€…æ€»èµ„æºï¼ˆå¦‚CPUã€å†…å­˜ç­‰æŠ½è±¡èµ„æºï¼‰
        self.attacker_total_res = config["attacker_total_res"]  # Cï¼šæ”»å‡»è€…æ€»èµ„æº
        self.host_importance = np.array(config["host_importance"])  # u_kï¼šæ¯å°ä¸»æœºçš„é‡è¦æ€§ï¼ˆåŸºäºå­˜å‚¨æ•°æ®é‡ï¼‰
        self.history_window = config["history_window"]  # Tï¼šçŠ¶æ€å†å²çª—å£é•¿åº¦
        self.max_steps = config["max_steps"]  # æ¯å›åˆæœ€å¤§æ­¥æ•°ï¼ˆè®ºæ–‡å®éªŒä¸­æ”¶æ•›å‰å‡<10000ï¼ŒğŸ”¶1-193ï¼‰
        
        # åˆå§‹åŒ–åŠ¨ä½œç©ºé—´ï¼šæ‰€æœ‰åˆæ³•çš„èµ„æºåˆ†é…ç­–ç•¥ï¼ˆğŸ”¶1-51ç­–ç•¥å®šä¹‰ï¼‰
        # é˜²å¾¡è€…ç­–ç•¥x=(x^1,...,x^N)ï¼Œæ»¡è¶³0â‰¤x^kâ‰¤Bä¸”âˆ‘x^kâ‰¤Bï¼›æ”»å‡»è€…ç­–ç•¥yåŒç†
        self.defender_actions = self._generate_resource_allocation_strategies(self.defender_total_res)
        self.attacker_actions = self._generate_resource_allocation_strategies(self.attacker_total_res)
        self.defender_action_dim = len(self.defender_actions)  # é˜²å¾¡è€…åŠ¨ä½œæ•°ï¼ˆç­–ç•¥æ€»æ•°ï¼‰
        self.attacker_action_dim = len(self.attacker_actions)  # æ”»å‡»è€…åŠ¨ä½œæ•°ï¼ˆç­–ç•¥æ€»æ•°ï¼‰
        
        # ç¯å¢ƒå†…éƒ¨çŠ¶æ€
        self.current_step = 0
        self.state: CyberState = None
        self.reset()

    def _generate_resource_allocation_strategies(self, total_res: int) -> List[np.ndarray]:
        """
        ç”Ÿæˆæ‰€æœ‰åˆæ³•çš„èµ„æºåˆ†é…ç­–ç•¥ï¼ˆè®ºæ–‡ğŸ”¶1-51ç­–ç•¥é›†åˆå®šä¹‰ï¼‰
        è¾“å…¥ï¼šæ€»èµ„æºé‡ï¼ˆå¦‚é˜²å¾¡è€…B=5ï¼‰
        è¾“å‡ºï¼šç­–ç•¥åˆ—è¡¨ï¼Œæ¯ä¸ªç­–ç•¥ä¸º[N,]æ•°ç»„ï¼ˆå¯¹åº”æ¯å°ä¸»æœºçš„èµ„æºåˆ†é…ï¼‰
        """
        strategies = []
        
        # é€’å½’ç”ŸæˆNç»´èµ„æºåˆ†é…ç»„åˆï¼ˆç¡®ä¿âˆ‘x^k â‰¤ total_resï¼‰
        def backtrack(remaining_res: int, host_idx: int, current_allocation: List[int]):
            if host_idx == self.num_hosts:
                # æœ€åä¸€å°ä¸»æœºåˆ†é…å‰©ä½™æ‰€æœ‰èµ„æºï¼ˆç¡®ä¿âˆ‘x^k = total_resï¼Œç®€åŒ–ç­–ç•¥ç©ºé—´ï¼‰
                current_allocation.append(remaining_res)
                strategies.append(np.array(current_allocation, dtype=np.float32))
                return
            # ä¸ºå½“å‰ä¸»æœºåˆ†é…0~remaining_resçš„èµ„æº
            for res in range(0, remaining_res + 1):
                backtrack(remaining_res - res, host_idx + 1, current_allocation + [res])
        
        backtrack(total_res, 1, [])
        return strategies

    def reset(self) -> CyberState:
        """
        é‡ç½®ç¯å¢ƒï¼šåˆå§‹åŒ–å†å²ç­–ç•¥çª—å£ï¼ˆè®ºæ–‡ğŸ”¶1-109çŠ¶æ€åˆå§‹åŒ–é€»è¾‘ï¼‰
        åˆå§‹ç­–ç•¥ï¼šå‡åŒ€åˆ†é…èµ„æºï¼ˆè®ºæ–‡ç¤ºä¾‹ä¸­å¸¸ç”¨çš„åŸºç¡€ç­–ç•¥ï¼Œå¦‚ğŸ”¶1-52ç¤ºä¾‹ï¼‰
        """
        self.current_step = 0
        # åˆå§‹é˜²å¾¡è€…ç­–ç•¥ï¼šå‡åŒ€åˆ†é…æ€»èµ„æºåˆ°Nå°ä¸»æœº
        init_def_strat = np.ones(self.num_hosts, dtype=np.float32) * (self.defender_total_res / self.num_hosts)
        # åˆå§‹æ”»å‡»è€…ç­–ç•¥ï¼šå‡åŒ€åˆ†é…æ€»èµ„æºåˆ°Nå°ä¸»æœº
        init_att_strat = np.ones(self.num_hosts, dtype=np.float32) * (self.attacker_total_res / self.num_hosts)
        # å¡«å……å†å²çª—å£ï¼ˆå‰Tè½®å‡ä¸ºåˆå§‹ç­–ç•¥ï¼‰
        init_history = [(init_def_strat, init_att_strat)] * self.history_window
        self.state = CyberState(history_strategies=init_history, history_window=self.history_window)
        return self.state

    def _calculate_utility(self, def_strat: np.ndarray, att_strat: np.ndarray) -> Tuple[float, float]:
        """
        è®¡ç®—æ”»é˜²åŒæ–¹æ•ˆç”¨ï¼ˆè®ºæ–‡ğŸ”¶1-58å…¬å¼ï¼‰
        é˜²å¾¡è€…æ•ˆç”¨U = âˆ‘u_k Â· sgn(âˆ‘x_i^k - âˆ‘y_j^k)ï¼ˆå•é˜²å¾¡è€…âˆ‘x_i^k=x^kï¼Œå•æ”»å‡»è€…âˆ‘y_j^k=y^kï¼‰
        æ”»å‡»è€…æ•ˆç”¨V = âˆ‘u_k Â· sgn(âˆ‘y_j^k - âˆ‘x_i^k) = -Uï¼ˆé›¶å’Œåšå¼ˆï¼‰
        """
        # è®¡ç®—æ¯å°ä¸»æœºçš„æ§åˆ¶æƒï¼ˆğŸ”¶1-57è§„åˆ™ï¼šèµ„æºå¤šè€…èµ¢ï¼Œç›¸ç­‰åˆ™å¹³å±€ï¼‰
        host_control = np.sign(def_strat - att_strat)  # sgn(a)=1(a>0), -1(a<0), 0(a=0)
        # è®¡ç®—æ•ˆç”¨
        def_utility = np.sum(self.host_importance * host_control)
        att_utility = np.sum(self.host_importance * (-host_control))  # é›¶å’Œåšå¼ˆï¼šæ”»å‡»è€…æ•ˆç”¨=é˜²å¾¡è€…æ•ˆç”¨çš„è´Ÿå€¼
        return def_utility, att_utility

    def _update_state_window(self, new_def_strat: np.ndarray, new_att_strat: np.ndarray):
        """
        æ›´æ–°å†å²çŠ¶æ€çª—å£ï¼ˆè®ºæ–‡ğŸ”¶1-115ã€Algorithm 1ç¬¬12è¡Œï¼‰
        é€»è¾‘ï¼šæ»‘åŠ¨çª—å£â†’åˆ é™¤æœ€æ—§è§‚æµ‹ï¼Œæ·»åŠ æœ€æ–°è§‚æµ‹ï¼šs^{t+1} = s^t âˆª {o^{t+1}} - {o^{t-T}}
        """
        new_history = self.state.history_strategies[1:]  # åˆ é™¤æœ€æ—§çš„1ä¸ªè§‚æµ‹
        new_history.append((new_def_strat, new_att_strat))  # æ·»åŠ æœ€æ–°çš„1ä¸ªè§‚æµ‹
        self.state.history_strategies = new_history

    def step(self, defender_action_idx: int, attacker_action_idx: int) -> Tuple[CyberState, float, float, bool, Dict]:
        """
        æ‰§è¡Œä¸€æ­¥åšå¼ˆï¼ˆè®ºæ–‡Algorithm 1æ ¸å¿ƒæ­¥éª¤ï¼šç¬¬7-13è¡Œï¼‰
        è¾“å…¥ï¼šé˜²å¾¡è€…/æ”»å‡»è€…åŠ¨ä½œç´¢å¼•ï¼ˆå¯¹åº”èµ„æºåˆ†é…ç­–ç•¥ï¼‰
        è¾“å‡ºï¼šæ–°çŠ¶æ€ã€é˜²å¾¡è€…å¥–åŠ±ã€æ”»å‡»è€…å¥–åŠ±ã€ç»ˆæ­¢æ ‡å¿—ã€ä¿¡æ¯å­—å…¸
        """
        self.current_step += 1
        
        # 1. è·å–å½“å‰æ”»é˜²ç­–ç•¥ï¼ˆä»åŠ¨ä½œç´¢å¼•æ˜ å°„åˆ°èµ„æºåˆ†é…æ–¹æ¡ˆï¼‰
        current_def_strat = self.defender_actions[defender_action_idx]
        current_att_strat = self.attacker_actions[attacker_action_idx]
        
        # 2. è®¡ç®—æ•ˆç”¨ï¼ˆè®ºæ–‡ğŸ”¶1-58ï¼‰
        def_utility, att_utility = self._calculate_utility(current_def_strat, current_att_strat)
        
        # 3. è®¡ç®—å¥–åŠ±ï¼ˆè®ºæ–‡ğŸ”¶1-115ï¼šå¥–åŠ±=æ•ˆç”¨/èµ„æºæ¶ˆè€—ï¼Œå•æ™ºèƒ½ä½“åœºæ™¯M=1ã€L=1ï¼‰
        def_resource_used = np.sum(current_def_strat)  # é˜²å¾¡è€…å½“å‰è½®èµ„æºæ¶ˆè€—
        att_resource_used = np.sum(current_att_strat)  # æ”»å‡»è€…å½“å‰è½®èµ„æºæ¶ˆè€—
        # é¿å…é™¤ä»¥0ï¼ˆèµ„æºæ¶ˆè€—ä¸º0æ—¶å¥–åŠ±ä¸º0ï¼Œä»£è¡¨"æ— åŠ¨ä½œ"æ— æ”¶ç›Šï¼‰
        def_reward = def_utility / def_resource_used if def_resource_used > 1e-6 else 0.0
        att_reward = att_utility / att_resource_used if att_resource_used > 1e-6 else 0.0
        
        # 4. æ›´æ–°å†å²çŠ¶æ€çª—å£ï¼ˆè®ºæ–‡Algorithm 1ç¬¬12è¡Œï¼‰
        self._update_state_window(current_def_strat, current_att_strat)
        
        # 5. æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶ï¼ˆè®ºæ–‡éšå«é€»è¾‘ï¼šæ­¥æ•°ç”¨å°½/èµ„æºè€—å°½ï¼‰
        done = (self.current_step >= self.max_steps 
                or def_resource_used < 1e-6  # é˜²å¾¡è€…æ— èµ„æºå¯ç”¨
                or att_resource_used < 1e-6)  # æ”»å‡»è€…æ— èµ„æºå¯ç”¨
        
        # 6. è®°å½•å…³é”®ä¿¡æ¯ï¼ˆè®ºæ–‡å®éªŒå…³æ³¨æŒ‡æ ‡ï¼šğŸ”¶1-142ã€ğŸ”¶1-160ï¼‰
        info = {
            "step": self.current_step,
            "def_utility": def_utility,
            "att_utility": att_utility,
            "def_strat": current_def_strat,
            "att_strat": current_att_strat,
            "def_resource_used": def_resource_used,
            "att_resource_used": att_resource_used
        }
        
        return self.state, def_reward, att_reward, done, info

    def get_state_vector(self) -> np.ndarray:
        """
        å°†çŠ¶æ€çª—å£è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥å‘é‡ï¼ˆè®ºæ–‡ğŸ”¶1-109çŠ¶æ€è¡¨ç¤ºï¼‰
        é€»è¾‘ï¼šå†å²Tè½®è§‚æµ‹â†’æ¯è½®å«Nç»´é˜²å¾¡ç­–ç•¥+Nç»´æ”»å‡»ç­–ç•¥â†’æ€»ç»´åº¦=T*(2*N)
        """
        state_flat = []
        for def_strat, att_strat in self.state.history_strategies:
            state_flat.extend(def_strat)  # æ‹¼æ¥é˜²å¾¡è€…ç­–ç•¥ï¼ˆNç»´ï¼‰
            state_flat.extend(att_strat)  # æ‹¼æ¥æ”»å‡»è€…ç­–ç•¥ï¼ˆNç»´ï¼‰
        return np.array(state_flat, dtype=np.float32)


# -------------------------- 3. è®ºæ–‡DQNç½‘ç»œæ¶æ„ï¼ˆåŸºäºğŸ”¶1-143ã€ğŸ”¶1-24ï¼‰ --------------------------
class CyberDQN(nn.Module):
    """
    è®ºæ–‡å®šä¹‰çš„DQNç½‘ç»œï¼šå››å±‚å…¨è¿æ¥ç¥ç»ç½‘ç»œï¼ˆè¾“å…¥å±‚+2ä¸ªéšè—å±‚+è¾“å‡ºå±‚ï¼‰
    å‚è€ƒğŸ”¶1-143ï¼š"four-layer fully connected neural network. Each of the two hidden layers has 1000 nodes"
    åŠŸèƒ½ï¼šè¾“å…¥çŠ¶æ€â†’è¾“å‡ºæ¯ä¸ªåŠ¨ä½œï¼ˆèµ„æºåˆ†é…ç­–ç•¥ï¼‰çš„Qå€¼
    """
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 1000):
        super().__init__()
        # è®ºæ–‡æ¶æ„ï¼šè¾“å…¥å±‚â†’éšè—å±‚1ï¼ˆ1000èŠ‚ç‚¹ï¼‰â†’éšè—å±‚2ï¼ˆ1000èŠ‚ç‚¹ï¼‰â†’è¾“å‡ºå±‚
        self.fc1 = nn.Linear(state_dim, hidden_dim)  # è¾“å…¥å±‚â†’éšè—å±‚1
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)  # éšè—å±‚1â†’éšè—å±‚2
        self.fc3 = nn.Linear(hidden_dim, action_dim)  # éšè—å±‚2â†’è¾“å‡ºå±‚ï¼ˆåŠ¨ä½œQå€¼ï¼‰

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­ï¼šçŠ¶æ€â†’Qå€¼ï¼ˆè®ºæ–‡æœªæ˜ç¡®æ¿€æ´»å‡½æ•°ï¼Œé‡‡ç”¨DQNå¸¸è§„ReLUï¼ŒğŸ”¶1-101éçº¿æ€§éœ€æ±‚ï¼‰"""
        x = F.relu(self.fc1(x))  # éšè—å±‚1 + ReLUæ¿€æ´»ï¼ˆæ•æ‰éçº¿æ€§å…³ç³»ï¼‰
        x = F.relu(self.fc2(x))  # éšè—å±‚2 + ReLUæ¿€æ´»
        return self.fc3(x)  # è¾“å‡ºå±‚ï¼šæ— æ¿€æ´»ï¼ˆQå€¼å¯ä¸ºä»»æ„å®æ•°ï¼‰


# -------------------------- 4. è®ºæ–‡DQNæ™ºèƒ½ä½“ï¼ˆåŸºäºğŸ”¶1-115ã€Algorithm 1ï¼‰ --------------------------
class CyberDQNAgent:
    """
    å•é˜²å¾¡è€…/æ”»å‡»è€…DQNæ™ºèƒ½ä½“ï¼ˆè®ºæ–‡Algorithm 1å®Œæ•´å®ç°ï¼‰
    æ ¸å¿ƒæœºåˆ¶ï¼šÎµ-è´ªå¿ƒç­–ç•¥ã€ç»éªŒå›æ”¾ã€ç›®æ ‡ç½‘ç»œæ›´æ–°ã€Qå€¼æ¢¯åº¦ä¸‹é™
    å‚è€ƒğŸ”¶1-115ï¼ˆAlgorithm 1ï¼‰ã€ğŸ”¶1-143ï¼ˆå‚æ•°è®¾ç½®ï¼‰
    """
    def __init__(self, state_dim: int, action_dim: int, agent_type: str = "defender"):
        self.agent_type = agent_type  # "defender"æˆ–"attacker"
        self.state_dim = state_dim    # è¾“å…¥çŠ¶æ€ç»´åº¦ï¼ˆT*(2*N)ï¼‰
        self.action_dim = action_dim  # è¾“å‡ºåŠ¨ä½œç»´åº¦ï¼ˆç­–ç•¥æ€»æ•°ï¼‰
        
        # 1. è®ºæ–‡DQNåŒç½‘ç»œæ¶æ„ï¼ˆä¸»ç½‘ç»œ+ç›®æ ‡ç½‘ç»œï¼ŒğŸ”¶1-101ã€ğŸ”¶1-115ï¼‰
        self.q_net = CyberDQN(state_dim, action_dim)  # ä¸»ç½‘ç»œï¼šå®æ—¶æ›´æ–°
        self.target_q_net = CyberDQN(state_dim, action_dim)  # ç›®æ ‡ç½‘ç»œï¼šå»¶è¿Ÿæ›´æ–°
        self.target_q_net.load_state_dict(self.q_net.state_dict())  # åˆå§‹åŒ–ç›®æ ‡ç½‘ç»œå‚æ•°
        
        # 2. è®ºæ–‡æŒ‡å®šè®­ç»ƒå‚æ•°ï¼ˆğŸ”¶1-143ï¼šç»å®éªŒéªŒè¯çš„æœ€ä¼˜å‚æ•°ï¼‰
        self.lr = 0.1  # å­¦ä¹ ç‡Î±=0.1
        self.gamma = 0.8  # æŠ˜æ‰£å› å­Î³=0.8
        self.epsilon = 0.8  # è´ªå¿ƒå‚æ•°Îµ=0.8ï¼ˆæ¢ç´¢æ¦‚ç‡ï¼‰
        self.epsilon_min = 0.01  # Îµæœ€å°é˜ˆå€¼ï¼ˆé¿å…å®Œå…¨åœæ­¢æ¢ç´¢ï¼‰
        self.epsilon_decay = 0.995  # Îµè¡°å‡ç‡ï¼ˆå¸¸è§„ä¼˜åŒ–ï¼Œè®ºæ–‡æœªæä½†éœ€ç¨³å®šè®­ç»ƒï¼‰
        self.batch_size = 3  # é‡‡æ ·æ‰¹æ¬¡å¤§å°m=3
        self.target_update_freq = 100  # ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡ï¼ˆå¸¸è§„ä¼˜åŒ–ï¼Œè®ºæ–‡æœªæï¼‰
        
        # 3. ç»éªŒå›æ”¾æ± ï¼ˆè®ºæ–‡ğŸ”¶1-115ç¬¬13è¡Œï¼ŒåŸºäºğŸ”¶1-101 DQNæ ¸å¿ƒæœºåˆ¶ï¼‰
        self.memory = deque(maxlen=10000)  # ç»éªŒæ± æœ€å¤§å®¹é‡ï¼ˆè®ºæ–‡æœªæŒ‡å®šï¼Œå–å¸¸è§„å€¼ï¼‰
        
        # 4. ä¼˜åŒ–å™¨ä¸è®­ç»ƒè®¡æ•°
        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=self.lr)  # è®ºæ–‡æœªæŒ‡å®šä¼˜åŒ–å™¨ï¼ŒAdamä¸ºDQNå¸¸è§„é€‰æ‹©
        self.update_count = 0  # ä¸»ç½‘ç»œæ›´æ–°è®¡æ•°ï¼ˆæ§åˆ¶ç›®æ ‡ç½‘ç»œæ›´æ–°ï¼‰

    def get_action(self, state: np.ndarray, training: bool = True) -> int:
        """
        é€‰æ‹©åŠ¨ä½œï¼šÎµ-è´ªå¿ƒç­–ç•¥ï¼ˆè®ºæ–‡Algorithm 1ç¬¬7è¡Œï¼‰
        è®­ç»ƒæ—¶ï¼šä»¥Îµæ¦‚ç‡éšæœºæ¢ç´¢ï¼Œ1-Îµæ¦‚ç‡é€‰Qå€¼æœ€å¤§åŠ¨ä½œï¼›è¯„ä¼°æ—¶ï¼šä»…é€‰Qå€¼æœ€å¤§åŠ¨ä½œ
        """
        if training and random.random() < self.epsilon:
            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©åŠ¨ä½œï¼ˆèµ„æºåˆ†é…ç­–ç•¥ï¼‰
            return random.randint(0, self.action_dim - 1)
        else:
            # åˆ©ç”¨ï¼šé€‰Qå€¼æœ€å¤§çš„åŠ¨ä½œï¼ˆè®ºæ–‡ğŸ”¶1-115ï¼šxt = argMaxx Q(st, x; Î¸)ï¼‰
            state_tensor = torch.FloatTensor(state).unsqueeze(0)  # (1, state_dim)
            with torch.no_grad():  # è¯„ä¼°æ—¶ä¸è®¡ç®—æ¢¯åº¦
                q_values = self.q_net(state_tensor)  # (1, action_dim)
            return q_values.argmax(dim=1).item()  # å–Qå€¼æœ€å¤§çš„åŠ¨ä½œç´¢å¼•

    def store_experience(self, state: np.ndarray, action: int, reward: float, next_state: np.ndarray, done: bool):
        """
        å­˜å‚¨ç»éªŒåˆ°å›æ”¾æ± ï¼ˆè®ºæ–‡Algorithm 1ç¬¬13è¡Œï¼‰
        ç»éªŒæ ¼å¼ï¼š(s_t, a_t, r_t, s_{t+1}, done_t)
        """
        self.memory.append((state, action, reward, next_state, done))

    def train_step(self) -> float:
        """
        è®­ç»ƒä¸»ç½‘ç»œï¼ˆè®ºæ–‡Algorithm 1ç¬¬14-16è¡Œï¼‰
        æ­¥éª¤ï¼šé‡‡æ ·ç»éªŒâ†’è®¡ç®—å½“å‰Qå€¼â†’è®¡ç®—ç›®æ ‡Qå€¼â†’MSEæŸå¤±â†’æ¢¯åº¦ä¸‹é™
        è¿”å›ï¼šå½“å‰è®­ç»ƒæŸå¤±
        """
        # ç»éªŒæ± æ ·æœ¬ä¸è¶³æ—¶ï¼Œä¸è®­ç»ƒï¼ˆé¿å…éšæœºè¯¯å·®ï¼‰
        if len(self.memory) < self.batch_size:
            return 0.0
        
        # 1. ä»ç»éªŒæ± é‡‡æ ·æ‰¹æ¬¡æ•°æ®ï¼ˆè®ºæ–‡Algorithm 1ç¬¬14è¡Œï¼‰
        batch = random.sample(self.memory, self.batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        
        # è½¬æ¢ä¸ºTensorï¼ˆé€‚é…PyTorchè®¡ç®—ï¼‰
        states = torch.FloatTensor(np.array(states))  # (batch_size, state_dim)
        actions = torch.LongTensor(actions).unsqueeze(1)  # (batch_size, 1)
        rewards = torch.FloatTensor(rewards).unsqueeze(1)  # (batch_size, 1)
        next_states = torch.FloatTensor(np.array(next_states))  # (batch_size, state_dim)
        dones = torch.BoolTensor(dones).unsqueeze(1)  # (batch_size, 1)ï¼ˆç»ˆæ­¢çŠ¶æ€æ ‡è®°ï¼‰

        # 2. è®¡ç®—å½“å‰Qå€¼ï¼ˆä¸»ç½‘ç»œè¾“å‡ºï¼Œè®ºæ–‡Algorithm 1ç¬¬15è¡Œï¼‰
        current_q = self.q_net(states).gather(1, actions)  # (batch_size, 1)ï¼šä»…å½“å‰åŠ¨ä½œçš„Qå€¼

        # 3. è®¡ç®—ç›®æ ‡Qå€¼ï¼ˆç›®æ ‡ç½‘ç»œè¾“å‡ºï¼Œè®ºæ–‡å…¬å¼ï¼šQj = rj + Î³Â·maxx Q(sj+1, x; Î¸)ï¼‰
        with torch.no_grad():  # ç›®æ ‡ç½‘ç»œä¸è®¡ç®—æ¢¯åº¦
            next_max_q = self.target_q_net(next_states).max(dim=1, keepdim=True)[0]  # (batch_size, 1)
            target_q = rewards + self.gamma * next_max_q * (~dones)  # ç»ˆæ­¢çŠ¶æ€ï¼šÎ³Â·next_max_q=0

        # 4. è®¡ç®—MSEæŸå¤±ï¼ˆè®ºæ–‡Algorithm 1ç¬¬16è¡Œï¼šæŸå¤±å‡½æ•°=1/m âˆ‘[Qj - Q(sj, xj; Î¸)]Â²ï¼‰
        loss = F.mse_loss(current_q, target_q)

        # 5. æ¢¯åº¦ä¸‹é™æ›´æ–°ä¸»ç½‘ç»œï¼ˆè®ºæ–‡Algorithm 1ç¬¬16è¡Œï¼‰
        self.optimizer.zero_grad()  # æ¸…ç©ºæ¢¯åº¦
        loss.backward()  # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
        self.optimizer.step()  # æ›´æ–°ä¸»ç½‘ç»œå‚æ•°

        # 6. è¡°å‡Îµï¼ˆæ§åˆ¶æ¢ç´¢-åˆ©ç”¨å¹³è¡¡ï¼Œå¸¸è§„ä¼˜åŒ–ï¼‰
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

        # 7. å®šæœŸæ›´æ–°ç›®æ ‡ç½‘ç»œï¼ˆå»¶è¿Ÿæ›´æ–°ï¼Œé¿å…è®­ç»ƒéœ‡è¡ï¼‰
        self.update_count += 1
        if self.update_count % self.target_update_freq == 0:
            self.target_q_net.load_state_dict(self.q_net.state_dict())

        return loss.item()

    def save_model(self, save_path: str):
        """ä¿å­˜æ¨¡å‹å‚æ•°ï¼ˆè®ºæ–‡æœªæï¼Œä½†ä¸ºå·¥ç¨‹åŒ–å¿…è¦åŠŸèƒ½ï¼‰"""
        torch.save(self.q_net.state_dict(), save_path)

    def load_model(self, load_path: str):
        """åŠ è½½æ¨¡å‹å‚æ•°ï¼ˆè®ºæ–‡æœªæï¼Œä½†ä¸ºå·¥ç¨‹åŒ–å¿…è¦åŠŸèƒ½ï¼‰"""
        self.q_net.load_state_dict(torch.load(load_path))
        self.target_q_net.load_state_dict(self.q_net.state_dict())


# -------------------------- 5. è®ºæ–‡å•æ”»é˜²åšå¼ˆè®­ç»ƒä¸è¯„ä¼°ï¼ˆåŸºäºğŸ”¶1-142ã€ğŸ”¶1-160ï¼‰ --------------------------
class CyberSecurityGame:
    """
    å•é˜²å¾¡è€…-å•æ”»å‡»è€…åšå¼ˆä¸»ç±»ï¼šæ•´åˆç¯å¢ƒä¸æ™ºèƒ½ä½“ï¼Œå®ç°è®­ç»ƒã€è¯„ä¼°ã€ç»“æœå¯è§†åŒ–
    å‚è€ƒğŸ”¶1-142ï¼ˆå®éªŒè®¾è®¡ï¼‰ã€ğŸ”¶1-160ï¼ˆè¯„ä¼°æŒ‡æ ‡ï¼šé˜²å¾¡è€…å¹³å‡æ•ˆç”¨ï¼‰
    """
    def __init__(self, config: Dict):
        self.config = config
        # åˆå§‹åŒ–ç¯å¢ƒ
        self.env = CyberSecurityEnvironment(config)
        # è®¡ç®—çŠ¶æ€ç»´åº¦ï¼šT*(2*N)ï¼ˆå†å²çª—å£Tï¼Œæ¯è½®2*Nç»´ç­–ç•¥ï¼‰
        self.state_dim = self.env.history_window * 2 * self.env.num_hosts
        # åˆå§‹åŒ–é˜²å¾¡è€…ä¸æ”»å‡»è€…æ™ºèƒ½ä½“
        self.defender_agent = CyberDQNAgent(
            state_dim=self.state_dim,
            action_dim=self.env.defender_action_dim,
            agent_type="defender"
        )
        self.attacker_agent = CyberDQNAgent(
            state_dim=self.state_dim,
            action_dim=self.env.attacker_action_dim,
            agent_type="attacker"
        )
        # è®­ç»ƒè®°å½•ï¼ˆè®ºæ–‡å®éªŒå…³æ³¨æŒ‡æ ‡ï¼šğŸ”¶1-160ã€ğŸ”¶1-192ï¼‰
        self.training_log = {
            "defender_rewards": [],  # é˜²å¾¡è€…æ¯å›åˆæ€»å¥–åŠ±
            "attacker_rewards": [],  # æ”»å‡»è€…æ¯å›åˆæ€»å¥–åŠ±
            "defender_utilities": [],  # é˜²å¾¡è€…æ¯å›åˆæ€»æ•ˆç”¨
            "attacker_utilities": [],  # æ”»å‡»è€…æ¯å›åˆæ€»æ•ˆç”¨
            "defender_losses": [],  # é˜²å¾¡è€…æ¯å›åˆå¹³å‡æŸå¤±
            "episode_lengths": []  # æ¯å›åˆæ­¥æ•°
        }

    def train_episode(self) -> Dict:
        """è®­ç»ƒä¸€ä¸ªå›åˆï¼ˆè®ºæ–‡å®éªŒçš„åŸºç¡€å•ä½ï¼ŒğŸ”¶1-142ï¼‰"""
        # é‡ç½®ç¯å¢ƒä¸çŠ¶æ€
        state = self.env.reset()
        state_vector = self.env.get_state_vector()
        # åˆå§‹åŒ–å›åˆç»Ÿè®¡
        ep_def_reward = 0.0
        ep_att_reward = 0.0
        ep_def_utility = 0.0
        ep_att_utility = 0.0
        ep_def_loss = 0.0
        ep_length = 0
        done = False

        while not done:
            # 1. é€‰æ‹©åŠ¨ä½œï¼ˆÎµ-è´ªå¿ƒï¼‰
            def_action = self.defender_agent.get_action(state_vector, training=True)
            att_action = self.attacker_agent.get_action(state_vector, training=True)
            
            # 2. æ‰§è¡ŒåŠ¨ä½œï¼Œè·å–åé¦ˆï¼ˆç¯å¢ƒstepï¼‰
            next_state, def_reward, att_reward, done, info = self.env.step(def_action, att_action)
            next_state_vector = self.env.get_state_vector()
            
            # 3. å­˜å‚¨ç»éªŒï¼ˆé˜²å¾¡è€…ä¸æ”»å‡»è€…åˆ†åˆ«å­˜å‚¨ï¼‰
            self.defender_agent.store_experience(state_vector, def_action, def_reward, next_state_vector, done)
            self.attacker_agent.store_experience(state_vector, att_action, att_reward, next_state_vector, done)
            
            # 4. è®­ç»ƒæ™ºèƒ½ä½“ï¼ˆä»…è®°å½•é˜²å¾¡è€…æŸå¤±ï¼Œè®ºæ–‡é‡ç‚¹å…³æ³¨é˜²å¾¡æ–¹æ€§èƒ½ï¼‰
            def_loss = self.defender_agent.train_step()
            self.attacker_agent.train_step()  # æ”»å‡»è€…è®­ç»ƒï¼ˆä¸è®°å½•æŸå¤±ï¼Œè®ºæ–‡ä»¥é˜²å¾¡æ–¹ä¸ºæ ¸å¿ƒï¼‰
            
            # 5. æ›´æ–°å›åˆç»Ÿè®¡
            state_vector = next_state_vector
            ep_def_reward += def_reward
            ep_att_reward += att_reward
            ep_def_utility += info["def_utility"]
            ep_att_utility += info["att_utility"]
            ep_def_loss += def_loss
            ep_length += 1

        # è®¡ç®—å›åˆå¹³å‡æŸå¤±ï¼ˆä»…é˜²å¾¡è€…ï¼‰
        avg_def_loss = ep_def_loss / ep_length if ep_length > 0 else 0.0
        # è®°å½•å›åˆæ•°æ®
        self.training_log["defender_rewards"].append(ep_def_reward)
        self.training_log["attacker_rewards"].append(ep_att_reward)
        self.training_log["defender_utilities"].append(ep_def_utility)
        self.training_log["attacker_utilities"].append(ep_att_utility)
        self.training_log["defender_losses"].append(avg_def_loss)
        self.training_log["episode_lengths"].append(ep_length)

        # è¿”å›å›åˆå…³é”®ä¿¡æ¯
        return {
            "episode": len(self.training_log["defender_rewards"]),
            "defender_reward": ep_def_reward,
            "defender_utility": ep_def_utility,
            "defender_loss": avg_def_loss,
            "episode_length": ep_length,
            "system_compromised": info.get("system_compromised", False)  # å…¼å®¹æ‰©å±•ï¼Œè®ºæ–‡å•æ”»é˜²æ— æ­¤æŒ‡æ ‡
        }

    def train(self, num_episodes: int = 1000):
        """è®­ç»ƒæŒ‡å®šå›åˆæ•°ï¼ˆè®ºæ–‡å®éªŒè®­ç»ƒé‡ï¼ŒğŸ”¶1-193ï¼šæ”¶æ•›å‰<10000å›åˆï¼‰"""
        print(f"=== è®ºæ–‡å•é˜²å¾¡è€…-å•æ”»å‡»è€…DQNè®­ç»ƒå¼€å§‹ï¼ˆ{num_episodes}å›åˆï¼‰ ===")
        print(f"è®ºæ–‡å‚æ•°ï¼šN={self.env.num_hosts}, B={self.env.defender_total_res}, C={self.env.attacker_total_res}, T={self.env.history_window}")
        print(f"è®­ç»ƒæ—¥å¿—ï¼ˆæ¯100å›åˆè¾“å‡ºä¸€æ¬¡ï¼‰ï¼š")
        
        for episode in range(1, num_episodes + 1):
            # è®­ç»ƒä¸€ä¸ªå›åˆ
            ep_info = self.train_episode()
            
            # æ¯100å›åˆè¾“å‡ºç»Ÿè®¡ï¼ˆè®ºæ–‡å®éªŒå¸¸ç”¨è¾“å‡ºé¢‘ç‡ï¼ŒğŸ”¶1-160ï¼‰
            if episode % 100 == 0 or episode == 1:
                # è®¡ç®—æœ€è¿‘100å›åˆçš„å¹³å‡æŒ‡æ ‡ï¼ˆè®ºæ–‡å®éªŒåˆ†ææ–¹å¼ï¼ŒğŸ”¶1-192ï¼‰
                recent_100_def_reward = np.mean(self.training_log["defender_rewards"][-100:])
                recent_100_def_utility = np.mean(self.training_log["defender_utilities"][-100:])
                recent_100_def_loss = np.mean(self.training_log["defender_losses"][-100:])
                recent_100_length = np.mean(self.training_log["episode_lengths"][-100:])
                
                print(f"å›åˆ {episode:4d} | "
                      f"é˜²å¾¡è€…å¹³å‡å¥–åŠ±ï¼š{recent_100_def_reward:6.2f} | "
                      f"é˜²å¾¡è€…å¹³å‡æ•ˆç”¨ï¼š{recent_100_def_utility:6.2f} | "
                      f"é˜²å¾¡è€…å¹³å‡æŸå¤±ï¼š{recent_100_def_loss:6.4f} | "
                      f"å¹³å‡å›åˆé•¿åº¦ï¼š{recent_100_length:4.1f}")
        
        # è®­ç»ƒç»“æŸåä¿å­˜æ¨¡å‹
        os.makedirs(self.config["output_dir"], exist_ok=True)
        self.defender_agent.save_model(os.path.join(self.config["output_dir"], "defender_dqn.pth"))
        self.attacker_agent.save_model(os.path.join(self.config["output_dir"], "attacker_dqn.pth"))
        print(f"\n=== è®­ç»ƒç»“æŸï¼æ¨¡å‹å·²ä¿å­˜è‡³ {self.config['output_dir']} ===")

    def evaluate(self, num_episodes: int = 100) -> Dict:
        """è¯„ä¼°æ™ºèƒ½ä½“æ€§èƒ½ï¼ˆè®ºæ–‡å®éªŒè¯„ä¼°ç¯èŠ‚ï¼ŒğŸ”¶1-142ã€ğŸ”¶1-192ï¼‰"""
        print(f"\n=== è®ºæ–‡å•é˜²å¾¡è€…-å•æ”»å‡»è€…DQNè¯„ä¼°å¼€å§‹ï¼ˆ{num_episodes}å›åˆï¼‰ ===")
        # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
        self.defender_agent.load_model(os.path.join(self.config["output_dir"], "defender_dqn.pth"))
        self.attacker_agent.load_model(os.path.join(self.config["output_dir"], "attacker_dqn.pth"))
        
        # åˆå§‹åŒ–è¯„ä¼°ç»Ÿè®¡
        eval_stats = {
            "defender_rewards": [],
            "defender_utilities": [],
            "episode_lengths": []
        }

        for _ in range(num_episodes):
            state = self.env.reset()
            state_vector = self.env.get_state_vector()
            ep_def_reward = 0.0
            ep_def_utility = 0.0
            ep_length = 0
            done = False

            while not done:
                # è¯„ä¼°æ—¶ä¸æ¢ç´¢ï¼ˆä»…é€‰Qå€¼æœ€å¤§åŠ¨ä½œï¼‰
                def_action = self.defender_agent.get_action(state_vector, training=False)
                att_action = self.attacker_agent.get_action(state_vector, training=False)
                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, def_reward, att_reward, done, info = self.env.step(def_action, att_action)
                # æ›´æ–°ç»Ÿè®¡
                state_vector = self.env.get_state_vector()
                ep_def_reward += def_reward
                ep_def_utility += info["def_utility"]
                ep_length += 1

            # è®°å½•è¯„ä¼°æ•°æ®
            eval_stats["defender_rewards"].append(ep_def_reward)
            eval_stats["defender_utilities"].append(ep_def_utility)
            eval_stats["episode_lengths"].append(ep_length)

        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡ï¼ˆè®ºæ–‡å…³æ³¨çš„å‡å€¼ä¸æ ‡å‡†å·®ï¼ŒğŸ”¶1-192ï¼‰
        result = {
            "avg_defender_reward": np.mean(eval_stats["defender_rewards"]),
            "std_defender_reward": np.std(eval_stats["defender_rewards"]),
            "avg_defender_utility": np.mean(eval_stats["defender_utilities"]),
            "std_defender_utility": np.std(eval_stats["defender_utilities"]),
            "avg_episode_length": np.mean(eval_stats["episode_lengths"]),
            "std_episode_length": np.std(eval_stats["episode_lengths"])
        }

        # è¾“å‡ºè¯„ä¼°ç»“æœï¼ˆè®ºæ–‡å®éªŒæŠ¥å‘Šæ ¼å¼ï¼ŒğŸ”¶1-192ï¼‰
        print(f"è¯„ä¼°ç»“æœï¼š")
        print(f"é˜²å¾¡è€…å¹³å‡å¥–åŠ±ï¼š{result['avg_defender_reward']:6.2f} Â± {result['std_defender_reward']:6.2f}")
        print(f"é˜²å¾¡è€…å¹³å‡æ•ˆç”¨ï¼š{result['avg_defender_utility']:6.2f} Â± {result['std_defender_utility']:6.2f}")
        print(f"å¹³å‡å›åˆé•¿åº¦ï¼š{result['avg_episode_length']:4.1f} Â± {result['std_episode_length']:4.1f}")
        print(f"=== è¯„ä¼°ç»“æŸ ===")
        return result

    def plot_training_curves(self, save_path: str = "training_curves.png"):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿ï¼ˆè®ºæ–‡å®éªŒå¯è§†åŒ–æ–¹å¼ï¼ŒğŸ”¶1-160ã€ğŸ”¶1-165ï¼‰"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        episodes = range(1, len(self.training_log["defender_rewards"]) + 1)
        
        # 1. é˜²å¾¡è€…å¥–åŠ±æ›²çº¿ï¼ˆè®ºæ–‡å›¾2(b)-(d)ã€å›¾3(b)-(d)é£æ ¼ï¼‰
        axes[0, 0].plot(episodes, self.training_log["defender_rewards"], alpha=0.7, label="Defender Reward")
        axes[0, 0].set_title("Defender Episode Reward (Paper Fig. 2-3 Style)", fontsize=12)
        axes[0, 0].set_xlabel("Episode", fontsize=10)
        axes[0, 0].set_ylabel("Total Reward", fontsize=10)
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. é˜²å¾¡è€…æ•ˆç”¨æ›²çº¿ï¼ˆè®ºæ–‡æ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡ï¼ŒğŸ”¶1-160ï¼‰
        axes[0, 1].plot(episodes, self.training_log["defender_utilities"], alpha=0.7, color="orange", label="Defender Utility")
        axes[0, 1].set_title("Defender Episode Utility (Paper Key Metric)", fontsize=12)
        axes[0, 1].set_xlabel("Episode", fontsize=10)
        axes[0, 1].set_ylabel("Total Utility", fontsize=10)
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. é˜²å¾¡è€…æŸå¤±æ›²çº¿ï¼ˆè®ºæ–‡æœªæ˜ç¡®ï¼Œä½†ä¸ºè®­ç»ƒç¨³å®šæ€§åˆ†æå¿…è¦ï¼‰
        axes[1, 0].plot(episodes, self.training_log["defender_losses"], alpha=0.7, color="red", label="Defender Loss")
        axes[1, 0].set_title("Defender Average Training Loss", fontsize=12)
        axes[1, 0].set_xlabel("Episode", fontsize=10)
        axes[1, 0].set_ylabel("MSE Loss", fontsize=10)
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. å›åˆé•¿åº¦æ›²çº¿ï¼ˆè®ºæ–‡å›¾2(b)-(d)è¾…åŠ©æŒ‡æ ‡ï¼‰
        axes[1, 1].plot(episodes, self.training_log["episode_lengths"], alpha=0.7, color="green", label="Episode Length")
        axes[1, 1].set_title("Episode Length (Paper Auxiliary Metric)", fontsize=12)
        axes[1, 1].set_xlabel("Episode", fontsize=10)
        axes[1, 1].set_ylabel("Step Count", fontsize=10)
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        # ä¿å­˜å›¾ç‰‡ï¼ˆè®ºæ–‡å›¾åˆ†è¾¨ç‡é£æ ¼ï¼ŒğŸ”¶1-17ã€ğŸ”¶1-18ï¼‰
        plt.tight_layout()
        plt.savefig(os.path.join(self.config["output_dir"], save_path), dpi=300, bbox_inches="tight")
        plt.close()
        print(f"è®­ç»ƒæ›²çº¿å·²ä¿å­˜è‡³ï¼š{os.path.join(self.config['output_dir'], save_path)}")


# -------------------------- 6. ä¸»å‡½æ•°ï¼ˆè®ºæ–‡å®éªŒå…¥å£ï¼‰ --------------------------
def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°ï¼ˆè®ºæ–‡å®éªŒå¯é…ç½®é¡¹ï¼‰
    parser = argparse.ArgumentParser(description="Zhu et al. 2023 - Single-Defender Single-Attacker DQN")
    parser.add_argument("--episodes", type=int, default=1000, help="è®­ç»ƒå›åˆæ•°ï¼ˆè®ºæ–‡å®éªŒ<10000ï¼‰")
    parser.add_argument("--eval_episodes", type=int, default=100, help="è¯„ä¼°å›åˆæ•°ï¼ˆè®ºæ–‡å®éªŒå¸¸ç”¨100ï¼‰")
    parser.add_argument("--num_hosts", type=int, default=2, help="ä¸»æœºæ•°é‡Nï¼ˆè®ºæ–‡ç¤ºä¾‹ç”¨2ï¼ŒğŸ”¶1-52ï¼‰")
    parser.add_argument("--def_res", type=int, default=5, help="é˜²å¾¡è€…æ€»èµ„æºBï¼ˆè®ºæ–‡ç¤ºä¾‹ç”¨5ï¼ŒğŸ”¶1-144ï¼‰")
    parser.add_argument("--att_res", type=int, default=5, help="æ”»å‡»è€…æ€»èµ„æºCï¼ˆè®ºæ–‡ç¤ºä¾‹ç”¨5ï¼ŒğŸ”¶1-144ï¼‰")
    parser.add_argument("--history_window", type=int, default=5, help="å†å²çª—å£Tï¼ˆè®ºæ–‡æœªæŒ‡å®šï¼Œå®éªŒå¯è°ƒï¼‰")
    parser.add_argument("--max_steps", type=int, default=100, help="æ¯å›åˆæœ€å¤§æ­¥æ•°ï¼ˆè®ºæ–‡å®éªŒ<10000ï¼‰")
    parser.add_argument("--output_dir", type=str, default="paper_dqn_output", help="è¾“å‡ºç›®å½•ï¼ˆæ¨¡å‹+æ›²çº¿ï¼‰")
    parser.add_argument("--do_train", action="store_true", help="æ‰§è¡Œè®­ç»ƒï¼ˆè®ºæ–‡å®éªŒæ ¸å¿ƒæ­¥éª¤ï¼‰")
    parser.add_argument("--do_eval", action="store_true", help="æ‰§è¡Œè¯„ä¼°ï¼ˆè®ºæ–‡å®éªŒéªŒè¯æ­¥éª¤ï¼‰")
    parser.add_argument("--do_plot", action="store_true", help="ç»˜åˆ¶è®­ç»ƒæ›²çº¿ï¼ˆè®ºæ–‡å®éªŒå¯è§†åŒ–ï¼‰")
    args = parser.parse_args()

    # æ„é€ è®ºæ–‡å®éªŒé…ç½®ï¼ˆä¸¥æ ¼å¯¹åº”è®ºæ–‡å‚æ•°ï¼ŒğŸ”¶1-143ã€ğŸ”¶1-144ï¼‰
    config = {
        "num_hosts": args.num_hosts,
        "defender_total_res": args.def_res,
        "attacker_total_res": args.att_res,
        "history_window": args.history_window,
        "max_steps": args.max_steps,
        "output_dir": args.output_dir,
        "host_importance": [2, 1]  # ä¸»æœºé‡è¦æ€§ï¼ˆè®ºæ–‡ç¤ºä¾‹ç”¨2å’Œ1ï¼ŒğŸ”¶1-144ï¼‰
    }

    # åˆå§‹åŒ–åšå¼ˆå®ä¾‹
    game = CyberSecurityGame(config)

    # æ‰§è¡Œè®­ç»ƒã€è¯„ä¼°ã€ç»˜å›¾ï¼ˆè®ºæ–‡å®éªŒå®Œæ•´æµç¨‹ï¼‰
    if args.do_train:
        game.train(num_episodes=args.episodes)
    if args.do_eval:
        game.evaluate(num_episodes=args.eval_episodes)
    if args.do_plot and args.do_train:
        game.plot_training_curves(save_path="paper_training_curves.png")


if __name__ == "__main__":
    main()